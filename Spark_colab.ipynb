{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2jAXrQon20+xvfxcH+y7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibe16/cc2-sparkML/blob/master/Spark_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PfALU6Dp37A",
        "colab_type": "text"
      },
      "source": [
        "# Práctica 4 Cloud Computing\n",
        "\n",
        "## *Procesamiento de grandes volúmenes de datos con Spark*\n",
        "\n",
        "![texto alternativo](https://joserzapata.github.io/es/post/pyspark-google-colab/featured.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY3rYWOWNDls",
        "colab_type": "text"
      },
      "source": [
        "# Instalar Spark\n",
        "\n",
        "Como Colab está en un ambiente linux se puede instalar con los comandos de la términal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCVJSJAcKkAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJnSkwG5NRuk",
        "colab_type": "text"
      },
      "source": [
        "Importamos las variables de entorno necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1-k2kC2Mvx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGhz4bnU8FD",
        "colab_type": "text"
      },
      "source": [
        "Iniciamos Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oGq04v3U6dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXkBXCz6Nfp2",
        "colab_type": "text"
      },
      "source": [
        "# Importar los datos \n",
        "\n",
        "Una vez seleccionadas las columnas del dataset vamos a importarlas a Colab para poder trabajar con ellas.\n",
        "\n",
        "Los datos se encuentran en una Carpeta de Google Drive. Para poder leerlos hay que montar Google Drive como un sistema de archivos normal, de esta manera se puede acceder a los datos desde python.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dySuzUBJM8aS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b852a974-3226-49f5-b22c-15e453a5f2fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/CC2/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4mpNG4zqa6X",
        "colab_type": "text"
      },
      "source": [
        "También se pueden subir al almacenaminto local de Colab desde el ordenador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJSgR82pQokZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Para subir ficheros desde el ordenador\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKkFLFZfRPLl",
        "colab_type": "text"
      },
      "source": [
        "# Script de Python\n",
        "\n",
        "Teniendo el dataset ya se puede trabajar con el script de Python desarrollado para la práctica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeuK_V05RduO",
        "colab_type": "text"
      },
      "source": [
        "## Funciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCj342D6Rfj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import sys\n",
        " \n",
        "from pyspark import SparkContext, SparkConf, SQLContext\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.classification import GBTClassifier, LogisticRegression, RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def createContext(appName= \"Practica 4: Irene Bejar Maldonado\"):\n",
        "    # create Spark context with Spark configuration  \n",
        "    conf = SparkConf().setAppName(appName)  \n",
        "    sc = SparkContext(conf=conf)\n",
        "    # Set Log info to display only error messages\n",
        "    sc.setLogLevel(\"ERROR\")\n",
        "    return sc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def createDataset(sc, path_data, path_header):\n",
        "    # Read header file\n",
        "    headers = sc.textFile(path_header).collect()\n",
        "    for line in headers:\n",
        "        if \"@inputs\" in line:\n",
        "            header_list = [x.replace(\",\",\"\").strip() for x in line.split()]\n",
        "\n",
        "    # Delete '@inputs' from the list\n",
        "    del header_list[0]\n",
        "    # Append column 'class' at the end\n",
        "    header_list.append(\"class\")\n",
        "\n",
        "    # Read data file\n",
        "    sqlContext = SQLContext(sc)\n",
        "    df = sqlContext.read.csv(path_data, header=False, sep=\",\", inferSchema=True)\n",
        "    print(df.schema)\n",
        "    # Rename columns\n",
        "    for i, colname in enumerate(df.columns):\n",
        "        df = df.withColumnRenamed(colname, header_list[i])\n",
        "\n",
        "    df = df.select(\"PSSM_r2_3_V\", \"AA_freq_global_L\", \"PSSM_central_-1_W\", \"PSSM_r2_1_H\", \"PSSM_central_0_Y\", \"PSSM_r1_3_N\", \"class\")\n",
        "    print(df.schema)\n",
        "\n",
        "    # Save the new small dataframe in a file\n",
        "    df.write.csv('./filteredC.small.training', header=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def loadSmallDataset(sc, path='./filteredC.small.training'):\n",
        "    sqlContext = SQLContext(sc)\n",
        "\n",
        "    # Prepare data schema\n",
        "    data_schema = StructType([\n",
        "        StructField(\"PSSM_r2_3_V\", FloatType(), True),\n",
        "        StructField(\"AA_freq_global_L\", FloatType(), True),\n",
        "        StructField(\"PSSM_central_-1_W\", FloatType(), True),\n",
        "        StructField(\"PSSM_r2_1_H\", FloatType(), True),\n",
        "        StructField(\"PSSM_central_0_Y\", FloatType(), True),\n",
        "        StructField(\"PSSM_r1_3_N\", FloatType(), True),\n",
        "        StructField(\"class\", IntegerType(), True)\n",
        "    ])\n",
        "\n",
        "    # Read the data using the schema\n",
        "    df = sqlContext.read.csv(path, header=True, sep=\",\", schema=data_schema)\n",
        "\n",
        "    print (\"--------------------------------------------------------------\\n\" + \n",
        "            \"Dataset loaded\\n\" +\n",
        "            \"--------------------------------------------------------------\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocessingData(sc, dataset):\n",
        "    # Transform all the feature columns into a single vector column\n",
        "    assemblerInputs = [\"PSSM_r2_3_V\", \"AA_freq_global_L\", \"PSSM_central_-1_W\", \"PSSM_r2_1_H\", \"PSSM_central_0_Y\", \"PSSM_r1_3_N\"]\n",
        "    vec_assembler = VectorAssembler(inputCols =assemblerInputs, outputCol='features')\n",
        "\n",
        "    # Add class column\n",
        "    # StringIndexer converts a single column into a index column (similar to a factor column in R)\n",
        "    indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
        "\n",
        "    # Normalization \n",
        "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "\n",
        "    # Apply transformations\n",
        "    pipeline = Pipeline (stages=[vec_assembler, indexer, scaler])\n",
        "    final_data = pipeline.fit(dataset).transform(dataset).select(\"scaled_features\", \"label\")\n",
        "\n",
        "    print (\"--------------------------------------------------------------\\n\" + \n",
        "            \"Preprocessed data\\n\")\n",
        "    final_data.printSchema()\n",
        "    print (\"--------------------------------------------------------------\\n\")\n",
        "\n",
        "    return final_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def RUS_data(dataset):\n",
        "    # The data is unbalanced. The ratio for each class is:\n",
        "    #  BEFORE Class 0=1375458 Class 1=687729 ratio=2\n",
        "    #  AFTER  Class 0=688517 Class 1=687729 ratio=1\n",
        "\n",
        "    class_0 = dataset.filter(dataset[\"label\"] == 0)\n",
        "    class_1 = dataset.filter(dataset[\"label\"] == 1)\n",
        "    ratio = float(class_0.count()/class_1.count())\n",
        "    \n",
        "    # Data has to be balanced. RUS is used for this.\n",
        "    sampled_majority = class_0.sample(False, float(1/ratio))\n",
        "    final_data = sampled_majority.unionAll(class_1)\n",
        "\n",
        "    print (\"--------------------------------------------------------------\\n\" + \n",
        "            \"Dataset balanced using RUS\\n\" +\n",
        "            \"--------------------------------------------------------------\\n\")\n",
        "\n",
        "    return final_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(dataset, train_size, test_size):\n",
        "    class_0 = dataset.filter(dataset[\"label\"] == 0)\n",
        "    class_1 = dataset.filter(dataset[\"label\"] == 1)\n",
        "\n",
        "    train_0, test_0 = class_0.randomSplit(weights=[train_size, test_size], seed=16)\n",
        "    train_1, test_1 = class_1.randomSplit(weights=[train_size, test_size], seed=16)\n",
        "\n",
        "    train = train_0.unionAll(train_1)\n",
        "    test = test_0.unionAll(test_1)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tune_model(estimator, param_grid, evaluator, train_data):\n",
        "    tvs = TrainValidationSplit(estimator=estimator,\n",
        "                           estimatorParamMaps=param_grid,\n",
        "                           evaluator=evaluator,\n",
        "                           # 80% of the data will be used for training, 20% for validation.\n",
        "                           trainRatio=0.8,\n",
        "                           seed=16)\n",
        "    \n",
        "    model = tvs.fit(train_data)\n",
        "    \n",
        "    # print results for each combination\n",
        "    for i, item in enumerate(model.getEstimatorParamMaps()):\n",
        "        grid = [\"%s: %s\" % (p.name, str(v)) for p, v in item.items()]\n",
        "        print(grid, model.getEvaluator().getMetricName(),\n",
        "              model.validationMetrics[i])\n",
        "\n",
        "\n",
        "    return model.bestModel\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_and_validation(estimator, train_data, test_data):\n",
        "    model = estimator.fit(train_data)\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator()\n",
        "    auc = evaluator.evaluate(model.transform(test_data))\n",
        "\n",
        "    print (\"--------------------------------------------------------------\\n\" + \n",
        "            \"Validation Result\\n\")\n",
        "    print(\"AUC sobre el conjunto de test: \", auc)\n",
        "    print (\"--------------------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_logisticRegresion(train_data, file_name):\n",
        "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaled_features\", seed=16)\n",
        "\n",
        "    paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.5, 1.0])\\\n",
        "    .addGrid(lr.maxIter, [10, 15])\\\n",
        "    .build()\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "    best_model = tune_model(lr, paramGrid, evaluator, train_data)\n",
        "    best_model.overwrite().save(file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_randomForest(train_data, file_name):\n",
        "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", seed=16)\n",
        "\n",
        "    paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(rf.numTrees, [10,15,20]) \\\n",
        "    .addGrid(rf.maxDepth, [3, 5])\\\n",
        "    .build()\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "    best_model = tune_model(rf, paramGrid, evaluator, train_data)\n",
        "    best_model.overwrite().save(file_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_GBT(train_data, file_name):\n",
        "    gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", seed=16)\n",
        "\n",
        "    paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(rf.maxIter, [5, 10, 15]) \\\n",
        "    .addGrid(rf.maxDepth, [2, 3, 10])\\\n",
        "    .build()\n",
        "\n",
        "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "    best_model = tune_model(gbt, paramGrid, evaluator, train_data)\n",
        "    best_model.overwrite().save(file_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl_Cx7nJRf7j",
        "colab_type": "text"
      },
      "source": [
        "## Preparación de los datos\n",
        "\n",
        "Se carga el dataset, se preprocesa y se le aplica RUS.\n",
        "También se divide en dos conjuntos: entrenamiento y validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHWG6HFKqA0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark_context = createContext() # Start Spark Session"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImUX0CAykQxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = \"/user/datasets/ecbdl14/ECBDL14_IR2.data\"\n",
        "header = \"/user/datasets/ecbdl14/ECBDL14_IR2.header\"\n",
        "\n",
        "path_to_load = root_dir+f'small_dataset.csv'\n",
        "\n",
        "# createDataset(sc=spark_context, path_data=data, path_header=header)\n",
        "df = loadSmallDataset(sc=spark_context, path=path_to_load)\n",
        "df = preprocessingData(spark_context, df)\n",
        "df = RUS_data(df)\n",
        "train, test = split_dataset(df, 70.0, 30.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxfuiV4EkY4C",
        "colab_type": "text"
      },
      "source": [
        "## Búsqueda de hiperparámetros\n",
        "\n",
        "Se entrenan los modelos con varias configuraciones para ver cual da mejor resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm6IxxSSRhwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Regresión logística\n",
        "lr = train_logisticRegresion(train, \"lr.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhGjAaLqkruw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest\n",
        "rf = train_randomForest(train, \"rf.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvARGFGYkrXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GBT\n",
        "gbt = train_GBT(train, \"gbt.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_rxpZYYRsjh",
        "colab_type": "text"
      },
      "source": [
        "## Resultados del entrenamiento\n",
        "\n",
        "Resultados del entrenamiento que se ha hecho para la búsqueda de parámetros.\n",
        "\n",
        "### Regresión logística\n",
        "\n",
        "``` python\n",
        "(['regParam: 0.1', 'elasticNetParam: 0.5', 'maxIter: 10'], 'areaUnderROC', 0.5)\n",
        "(['regParam: 0.1', 'elasticNetParam: 0.5', 'maxIter: 15'], 'areaUnderROC', 0.5)\n",
        "(['regParam: 0.1', 'elasticNetParam: 1.0', 'maxIter: 10'], 'areaUnderROC', 0.5)\n",
        "(['regParam: 0.1', 'elasticNetParam: 1.0', 'maxIter: 15'], 'areaUnderROC', 0.5)\n",
        "(['regParam: 0.01', 'elasticNetParam: 0.5', 'maxIter: 10'], 'areaUnderROC', 0.5255320922773915)\n",
        "(['regParam: 0.01', 'elasticNetParam: 0.5', 'maxIter: 15'], 'areaUnderROC', 0.5269092508177255)\n",
        "(['regParam: 0.01', 'elasticNetParam: 1.0', 'maxIter: 10'], 'areaUnderROC', 0.5341054461968381)\n",
        "(['regParam: 0.01', 'elasticNetParam: 1.0', 'maxIter: 15'], 'areaUnderROC', 0.5352423263189457)\n",
        "```\n",
        "\n",
        "### Random Forest\n",
        "\n",
        "```python\n",
        "(['maxDepth: 3', 'numTrees: 10'], 'areaUnderROC', 0.5604672226801575)\n",
        "(['maxDepth: 3', 'numTrees: 15'], 'areaUnderROC', 0.5624006470644223)\n",
        "(['maxDepth: 3', 'numTrees: 20'], 'areaUnderROC', 0.5599379950693529)\n",
        "(['maxDepth: 5', 'numTrees: 10'], 'areaUnderROC', 0.5701677290767797)\n",
        "(['maxDepth: 5', 'numTrees: 15'], 'areaUnderROC', 0.5698462502705504)\n",
        "(['maxDepth: 5', 'numTrees: 20'], 'areaUnderROC', 0.568444386126271)\n",
        "(['maxDepth: 10', 'numTrees: 10'], 'areaUnderROC', 0.5849019161955297)\n",
        "(['maxDepth: 10', 'numTrees: 15'], 'areaUnderROC', 0.5852199319212533)\n",
        "(['maxDepth: 10', 'numTrees: 20'], 'areaUnderROC', 0.5857663904229639)\n",
        "```\n",
        "\n",
        "\n",
        "### Gradient Boost Trees\n",
        "\n",
        "```python\n",
        "['maxIter: 5', 'maxDepth: 2'] areaUnderROC 0.5599631877122353\n",
        "['maxIter: 5', 'maxDepth: 3'] areaUnderROC 0.5656066571484597\n",
        "['maxIter: 5', 'maxDepth: 10'] areaUnderROC 0.5849454688788238\n",
        "['maxIter: 10', 'maxDepth: 2'] areaUnderROC 0.5640069604974786\n",
        "['maxIter: 10', 'maxDepth: 3'] areaUnderROC 0.5704346140656521\n",
        "['maxIter: 10', 'maxDepth: 10'] areaUnderROC 0.5911215374107248\n",
        "['maxIter: 15', 'maxDepth: 2'] areaUnderROC 0.567284795231468\n",
        "['maxIter: 15', 'maxDepth: 3'] areaUnderROC 0.5741910996336299\n",
        "['maxIter: 15', 'maxDepth: 10'] areaUnderROC 0.5946650969503753\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvJDLK24lPRu",
        "colab_type": "text"
      },
      "source": [
        "# Entrenamiento y validación del modelo\n",
        "\n",
        "El modelo que mejor resultado ha obtenido es GBT con un AUC de 0.594 con la siguiente configuración:\n",
        "\n",
        "* maxIter = 15 \n",
        "* maxDepth = 10\n",
        "\n",
        "Vamos a entrenarlo sobre todo el conjunto de train con esta configuración y a validarlo sobre el conjunto de test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XmCantzmD1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "17ef471a-9489-4c15-c071-c27d3334f68e"
      },
      "source": [
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", maxIter=15, maxDepth=10, seed=16)\n",
        "train_and_validation(gbt, train, test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------\n",
            "Validation Result\n",
            "\n",
            "AUC sobre el conjunto de test:  0.5971556448620811\n",
            "--------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmuAFWtepYW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark_context.stop() #To finalize Spark session"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LvqRLbQphGi",
        "colab_type": "text"
      },
      "source": [
        "El resultado final sobre el conjunto de TEST es un **0.597 de AUC**."
      ]
    }
  ]
}